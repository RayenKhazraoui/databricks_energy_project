{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd03f31-13bd-45ee-8d89-6f5b36d35b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists data.gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4066ce3b-30b2-45a3-a608-3498b1c51d97",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767796844344}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Extract position and quantity from XML Points"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, explode, expr, to_timestamp, regexp_extract, from_xml, schema_of_xml, array\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def parse_xml_row(df_input: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Parse XML data using Spark's native XML parser to handle multiple Periods.\n",
    "    \n",
    "#     Args:\n",
    "#         df_input: DataFrame with columns: filename, current_timestamp, xml_string\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with one row per Point, with correct position_timestamp per Period\n",
    "#     \"\"\"\n",
    "#     # Extract metadata\n",
    "#     df_base = df_input.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"current_timestamp\").alias(\"ingestion_timestamp\"),\n",
    "#         to_timestamp(regexp_extract(col(\"filename\"), r\"(\\d{12})\\.xml$\", 1), \"yyyyMMddHHmm\").alias(\"file_timestamp\"),\n",
    "#         col(\"xml_string\")\n",
    "#     )\n",
    "    \n",
    "#     # Infer schema from a sample XML\n",
    "#     sample_xml = df_base.select(\"xml_string\").first()[0]\n",
    "#     xml_schema = schema_of_xml(sample_xml)\n",
    "    \n",
    "#     # Parse XML into struct\n",
    "#     df_parsed_xml = df_base.withColumn(\"parsed\", from_xml(col(\"xml_string\"), xml_schema))\n",
    "    \n",
    "#     # Check if Period is an array or struct by inspecting the schema\n",
    "#     period_field = df_parsed_xml.select(\"parsed.TimeSeries.Period\").schema.fields[0]\n",
    "#     is_period_array = str(period_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "#     # Extract the TimeSeries.Period and explode it\n",
    "#     if is_period_array:\n",
    "#         # Period is already an array\n",
    "#         df_period_data = df_parsed_xml.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             col(\"file_timestamp\"),\n",
    "#             explode(col(\"parsed.TimeSeries.Period\")).alias(\"period\")\n",
    "#         )\n",
    "#     else:\n",
    "#         # Period is a struct - wrap in array first\n",
    "#         df_period_data = df_parsed_xml.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             col(\"file_timestamp\"),\n",
    "#             explode(array(col(\"parsed.TimeSeries.Period\"))).alias(\"period\")\n",
    "#         )\n",
    "    \n",
    "#     # Extract interval_start and interval_end from each Period\n",
    "#     df_period_with_intervals = df_period_data.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"file_timestamp\"),\n",
    "#         to_timestamp(col(\"period.timeInterval.start\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_start\"),\n",
    "#         to_timestamp(col(\"period.timeInterval.end\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_end\"),\n",
    "#         col(\"period.Point\").alias(\"points_array\")\n",
    "#     )\n",
    "    \n",
    "#     # Explode the Point array to get one row per Point\n",
    "#     df_points_exploded = df_period_with_intervals.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"file_timestamp\"),\n",
    "#         col(\"interval_start\"),\n",
    "#         col(\"interval_end\"),\n",
    "#         explode(col(\"points_array\")).alias(\"point\")\n",
    "#     )\n",
    "    \n",
    "#     # Extract position and quantity from each Point\n",
    "#     df_result = df_points_exploded.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"file_timestamp\"),\n",
    "#         col(\"interval_start\"),\n",
    "#         col(\"interval_end\"),\n",
    "#         col(\"point.position\").cast(\"int\").alias(\"position\"),\n",
    "#         col(\"point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "#     )\n",
    "    \n",
    "#     # Calculate position_timestamp: interval_start + (position - 1) * 15 minutes\n",
    "#     df_final = df_result.withColumn(\n",
    "#         \"position_timestamp\",\n",
    "#         expr(\"interval_start + make_interval(0, 0, 0, 0, 0, (position - 1) * 15)\")\n",
    "#     )\n",
    "    \n",
    "#     return df_final\n",
    "\n",
    "# # ===== CHANGE THIS VARIABLE TO TEST DIFFERENT ROWS =====\n",
    "# row_number = 5  # Change this to 1, 2, 3, 4, etc.\n",
    "# # ========================================================\n",
    "\n",
    "# # Get the specified row from bronze table (sorted by filename for consistency)\n",
    "# df_all_bronze = spark.table(\"data.bronze.bronze_A16\").orderBy(\"filename\")\n",
    "# df_bronze_row = df_all_bronze.limit(row_number).tail(1)\n",
    "# df_bronze_single = spark.createDataFrame(df_bronze_row, df_all_bronze.schema)\n",
    "\n",
    "# # Parse the XML\n",
    "# df_parsed = parse_xml_row(df_bronze_single)\n",
    "\n",
    "# print(f\"Testing row {row_number} from bronze_A16 (sorted by filename)\")\n",
    "# print(f\"Filename: {df_bronze_single.first()['filename']}\")\n",
    "# print(f\"Total Points extracted: {df_parsed.count()}\")\n",
    "# print(\"\\nParsed data with position_timestamp:\")\n",
    "# df_parsed.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c11920a-219e-4c85-b3a5-a5aa49f638d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create silver table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS data.silver;\n",
    "\n",
    "CREATE table if not EXISTS  data.silver.silver_A16 (\n",
    "  filename STRING,\n",
    "  ingestion_timestamp TIMESTAMP,\n",
    "  file_timestamp TIMESTAMP,\n",
    "  interval_start TIMESTAMP,\n",
    "  interval_end TIMESTAMP,\n",
    "  position INT,\n",
    "  quantity DOUBLE,\n",
    "  position_timestamp TIMESTAMP\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd7ae12-15a8-4d34-a985-7440ea5453e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Setup streaming source"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, explode, expr, to_timestamp, regexp_extract, from_xml, schema_of_xml, array\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Define parse_xml_row function (needed in same cell for foreachBatch)\n",
    "def parse_xml_row(df_input: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse XML data using Spark's native XML parser to handle multiple Periods.\n",
    "    \"\"\"\n",
    "    df_base = df_input.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"current_timestamp\").alias(\"ingestion_timestamp\"),\n",
    "        to_timestamp(regexp_extract(col(\"filename\"), r\"(\\d{12})\\.xml$\", 1), \"yyyyMMddHHmm\").alias(\"file_timestamp\"),\n",
    "        col(\"xml_string\")\n",
    "    )\n",
    "    \n",
    "    sample_xml = df_base.select(\"xml_string\").first()[0]\n",
    "    xml_schema = schema_of_xml(sample_xml)\n",
    "    df_parsed_xml = df_base.withColumn(\"parsed\", from_xml(col(\"xml_string\"), xml_schema))\n",
    "    \n",
    "    period_field = df_parsed_xml.select(\"parsed.TimeSeries.Period\").schema.fields[0]\n",
    "    is_period_array = str(period_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "    if is_period_array:\n",
    "        df_period_data = df_parsed_xml.select(\n",
    "            col(\"filename\"), col(\"ingestion_timestamp\"), col(\"file_timestamp\"),\n",
    "            explode(col(\"parsed.TimeSeries.Period\")).alias(\"period\")\n",
    "        )\n",
    "    else:\n",
    "        df_period_data = df_parsed_xml.select(\n",
    "            col(\"filename\"), col(\"ingestion_timestamp\"), col(\"file_timestamp\"),\n",
    "            explode(array(col(\"parsed.TimeSeries.Period\"))).alias(\"period\")\n",
    "        )\n",
    "    \n",
    "    df_period_with_intervals = df_period_data.select(\n",
    "        col(\"filename\"), col(\"ingestion_timestamp\"), col(\"file_timestamp\"),\n",
    "        to_timestamp(col(\"period.timeInterval.start\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_start\"),\n",
    "        to_timestamp(col(\"period.timeInterval.end\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_end\"),\n",
    "        col(\"period.Point\").alias(\"points_array\")\n",
    "    )\n",
    "    \n",
    "    df_points_exploded = df_period_with_intervals.select(\n",
    "        col(\"filename\"), col(\"ingestion_timestamp\"), col(\"file_timestamp\"),\n",
    "        col(\"interval_start\"), col(\"interval_end\"),\n",
    "        explode(col(\"points_array\")).alias(\"point\")\n",
    "    )\n",
    "    \n",
    "    df_result = df_points_exploded.select(\n",
    "        col(\"filename\"), col(\"ingestion_timestamp\"), col(\"file_timestamp\"),\n",
    "        col(\"interval_start\"), col(\"interval_end\"),\n",
    "        col(\"point.position\").cast(\"int\").alias(\"position\"),\n",
    "        col(\"point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_result.withColumn(\n",
    "        \"position_timestamp\",\n",
    "        expr(\"interval_start + make_interval(0, 0, 0, 0, 0, (position - 1) * 15)\")\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Define the process_batch function for foreachBatch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Process a micro-batch of bronze data and MERGE into silver table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing batch {batch_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    row_count = batch_df.count()\n",
    "    print(f\"Batch contains {row_count} bronze rows\")\n",
    "    \n",
    "    if row_count == 0:\n",
    "        print(\"Empty batch, skipping...\")\n",
    "        return\n",
    "    \n",
    "    silver_table = DeltaTable.forName(spark, \"data.silver.silver_A16\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    for row in batch_df.toLocalIterator():\n",
    "        processed_count += 1\n",
    "        filename = row['filename']\n",
    "        print(f\"\\n  [{processed_count}/{row_count}] Processing: {filename}\")\n",
    "        \n",
    "        df_single_row = spark.createDataFrame([row], batch_df.schema)\n",
    "        df_parsed = parse_xml_row(df_single_row)\n",
    "        \n",
    "        silver_table.alias(\"target\").merge(\n",
    "            df_parsed.alias(\"source\"),\n",
    "            \"target.position_timestamp = source.position_timestamp\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"source.file_timestamp > target.file_timestamp\",\n",
    "            set={\n",
    "                \"filename\": \"source.filename\",\n",
    "                \"ingestion_timestamp\": \"source.ingestion_timestamp\",\n",
    "                \"file_timestamp\": \"source.file_timestamp\",\n",
    "                \"interval_start\": \"source.interval_start\",\n",
    "                \"interval_end\": \"source.interval_end\",\n",
    "                \"position\": \"source.position\",\n",
    "                \"quantity\": \"source.quantity\",\n",
    "                \"position_timestamp\": \"source.position_timestamp\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "        \n",
    "        print(f\"    ✓ MERGE completed\")\n",
    "    \n",
    "    print(f\"\\n✓ Batch {batch_id} completed: {processed_count} files processed\")\n",
    "\n",
    "print(\"✓ parse_xml_row and process_batch functions defined\")\n",
    "print(\"Ready to use with foreachBatch in streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c4cae3-c6c7-4a71-905f-21ceb21ef95d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Setup and start streaming"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Setup streaming from bronze to silver\n",
    "print(\"Setting up structured streaming...\")\n",
    "\n",
    "# Define checkpoint location\n",
    "checkpoint_location = \"/Volumes/source/source_schema/source_volume/checkpoints/bronze_to_silver_A16\"\n",
    "\n",
    "# Read bronze table as a stream\n",
    "df_bronze_stream = spark.readStream.table(\"data.bronze.bronze_A16\")\n",
    "\n",
    "print(f\"✓ Streaming source configured\")\n",
    "print(f\"Checkpoint location: {checkpoint_location}\")\n",
    "\n",
    "# Start the streaming query with foreachBatch\n",
    "print(\"\\nStarting streaming query...\")\n",
    "print(\"Mode: Batch (trigger availableNow - processes all available data once)\\n\")\n",
    "\n",
    "query = df_bronze_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to finish processing\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Streaming query completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll unprocessed bronze rows have been processed to silver.\")\n",
    "print(\"Next run will only process new rows added after this checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9957d327-e878-4575-b6d8-295afd176dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from data.silver.silver_A16 order by position_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45447eef-9e7e-4e32-b359-28d6eb76a0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6207694231928380,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_to_Silver_A16",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
