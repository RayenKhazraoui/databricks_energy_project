{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d70953-ee4b-4b24-b243-fc3158c54bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Load bronze data for testing\n",
    "# row_number = 1\n",
    "# df_all_bronze = spark.table(\"data.bronze.bronze_A75\").orderBy(\"filename\")\n",
    "# df_bronze_row = df_all_bronze.limit(row_number).tail(1)\n",
    "# df_bronze_single = spark.createDataFrame(df_bronze_row, df_all_bronze.schema)\n",
    "\n",
    "# df_bronze_single.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0ba247-28cf-44b8-bc27-ffbf990ac32f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract TimeSeries from XML"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, explode, to_timestamp, regexp_extract, from_xml, schema_of_xml, array\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def extract_timeseries(df_input: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Extract each TimeSeries from XML as a separate row.\n",
    "    \n",
    "#     Args:\n",
    "#         df_input: DataFrame with columns: filename, current_timestamp, xml_string\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with one row per TimeSeries, including the TimeSeries struct and psrType\n",
    "#     \"\"\"\n",
    "#     # Extract metadata\n",
    "#     df_base = df_input.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"current_timestamp\").alias(\"ingestion_timestamp\"),\n",
    "#         col(\"xml_string\")\n",
    "#     )\n",
    "    \n",
    "#     # Infer schema from a sample XML\n",
    "#     sample_xml = df_base.select(\"xml_string\").first()[0]\n",
    "#     xml_schema = schema_of_xml(sample_xml)\n",
    "    \n",
    "#     # Parse XML into struct\n",
    "#     df_parsed_xml = df_base.withColumn(\"parsed\", from_xml(col(\"xml_string\"), xml_schema))\n",
    "    \n",
    "#     # Check if TimeSeries is an array or struct\n",
    "#     timeseries_field = df_parsed_xml.select(\"parsed.TimeSeries\").schema.fields[0]\n",
    "#     is_timeseries_array = str(timeseries_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "#     # Extract and explode TimeSeries\n",
    "#     if is_timeseries_array:\n",
    "#         # TimeSeries is already an array\n",
    "#         df_timeseries = df_parsed_xml.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             explode(col(\"parsed.TimeSeries\")).alias(\"timeseries\")\n",
    "#         )\n",
    "#     else:\n",
    "#         # TimeSeries is a struct - wrap in array first\n",
    "#         df_timeseries = df_parsed_xml.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             explode(array(col(\"parsed.TimeSeries\"))).alias(\"timeseries\")\n",
    "#         )\n",
    "    \n",
    "#     # Extract psrType from the timeseries struct\n",
    "#     df_timeseries_with_psrtype = df_timeseries.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"timeseries.MktPSRType.psrType\").alias(\"psrType\"),\n",
    "#         col(\"timeseries\")\n",
    "#     )\n",
    "    \n",
    "#     return df_timeseries_with_psrtype\n",
    "\n",
    "# # Test with the same row\n",
    "# row_number = 1\n",
    "# df_all_bronze = spark.table(\"data.bronze.bronze_A75\").orderBy(\"filename\")\n",
    "# df_bronze_row = df_all_bronze.limit(row_number).tail(1)\n",
    "# df_bronze_single = spark.createDataFrame(df_bronze_row, df_all_bronze.schema)\n",
    "\n",
    "# # Extract TimeSeries\n",
    "# df_timeseries = extract_timeseries(df_bronze_single)\n",
    "\n",
    "# df_timeseries.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff0ca5f-8227-4273-87b2-9e9b16660703",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Periods from TimeSeries"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, explode, array, to_timestamp\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def extract_periods(df_timeseries: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Extract each Period from TimeSeries as a separate row.\n",
    "    \n",
    "#     Args:\n",
    "#         df_timeseries: DataFrame with columns: filename, ingestion_timestamp, psrType, timeseries\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with one row per Period, including the Period struct and interval start/end\n",
    "#     \"\"\"\n",
    "#     # Check if Period is an array or struct by inspecting the schema\n",
    "#     period_field = df_timeseries.select(\"timeseries.Period\").schema.fields[0]\n",
    "#     is_period_array = str(period_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "#     # Extract and explode Period\n",
    "#     if is_period_array:\n",
    "#         # Period is already an array\n",
    "#         df_periods = df_timeseries.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             col(\"psrType\"),\n",
    "#             explode(col(\"timeseries.Period\")).alias(\"period\")\n",
    "#         )\n",
    "#     else:\n",
    "#         # Period is a struct - wrap in array first\n",
    "#         df_periods = df_timeseries.select(\n",
    "#             col(\"filename\"),\n",
    "#             col(\"ingestion_timestamp\"),\n",
    "#             col(\"psrType\"),\n",
    "#             explode(array(col(\"timeseries.Period\"))).alias(\"period\")\n",
    "#         )\n",
    "    \n",
    "#     # Extract interval_start and interval_end from the period\n",
    "#     df_periods_with_intervals = df_periods.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"psrType\"),\n",
    "#         to_timestamp(col(\"period.timeInterval.start\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_start\"),\n",
    "#         to_timestamp(col(\"period.timeInterval.end\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_end\"),\n",
    "#         col(\"period\")\n",
    "#     )\n",
    "    \n",
    "#     return df_periods_with_intervals\n",
    "\n",
    "# # Extract Periods from the timeseries\n",
    "# df_periods = extract_periods(df_timeseries)\n",
    "\n",
    "# df_periods.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa0ecbe-dfd3-460c-96ae-854e510f82f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode Points and Calculate Timestamps"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, explode, expr\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def explode_points(df_periods: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Explode Points from each Period and calculate position_timestamp.\n",
    "    \n",
    "#     Args:\n",
    "#         df_periods: DataFrame with columns: filename, ingestion_timestamp, psrType, interval_start, interval_end, period\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with one row per Point, including position_timestamp\n",
    "#     \"\"\"\n",
    "#     # Explode the Point array to get one row per Point\n",
    "#     df_points_exploded = df_periods.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"psrType\"),\n",
    "#         col(\"interval_start\"),\n",
    "#         col(\"interval_end\"),\n",
    "#         explode(col(\"period.Point\")).alias(\"point\")\n",
    "#     )\n",
    "    \n",
    "#     # Extract position and quantity from each Point\n",
    "#     df_result = df_points_exploded.select(\n",
    "#         col(\"filename\"),\n",
    "#         col(\"ingestion_timestamp\"),\n",
    "#         col(\"psrType\"),\n",
    "#         col(\"interval_start\"),\n",
    "#         col(\"interval_end\"),\n",
    "#         col(\"point.position\").cast(\"int\").alias(\"position\"),\n",
    "#         col(\"point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "#     )\n",
    "    \n",
    "#     # Calculate position_timestamp: interval_start + (position - 1) * 15 minutes\n",
    "#     df_final = df_result.withColumn(\n",
    "#         \"position_timestamp\",\n",
    "#         expr(\"interval_start + make_interval(0, 0, 0, 0, 0, (position - 1) * 15)\")\n",
    "#     )\n",
    "    \n",
    "#     return df_final\n",
    "\n",
    "# # Explode points and calculate timestamps\n",
    "# df_final = explode_points(df_periods)\n",
    "\n",
    "# df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fbb861-d403-4118-b8e6-e1796a2cc202",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pivot on psrType"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, first\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def pivot_psrtype(df_final: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Pivot the dataframe so each psrType becomes a column.\n",
    "    \n",
    "#     Args:\n",
    "#         df_final: DataFrame with columns: filename, ingestion_timestamp, psrType, interval_start, interval_end, position, quantity, position_timestamp\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame pivoted on psrType with position_timestamp as the key\n",
    "#     \"\"\"\n",
    "#     # Group by position_timestamp and metadata columns, then pivot on psrType\n",
    "#     df_pivoted = df_final.groupBy(\n",
    "#         \"position_timestamp\",\n",
    "#         \"filename\",\n",
    "#         \"ingestion_timestamp\",\n",
    "#         \"interval_start\",\n",
    "#         \"interval_end\"\n",
    "#     ).pivot(\"psrType\").agg(first(\"quantity\"))\n",
    "    \n",
    "#     return df_pivoted\n",
    "\n",
    "# # Pivot the dataframe\n",
    "# df_pivoted = pivot_psrtype(df_final)\n",
    "\n",
    "# df_pivoted.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca37523-202d-410a-9f56-5f59557e69ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define parse and upsert functions"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, explode, expr, to_timestamp, regexp_extract, from_xml, schema_of_xml, array, first\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Define modular parsing functions\n",
    "def extract_timeseries(df_input: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract each TimeSeries from XML as a separate row.\n",
    "    \"\"\"\n",
    "    df_base = df_input.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"current_timestamp\").alias(\"ingestion_timestamp\"),\n",
    "        to_timestamp(regexp_extract(col(\"filename\"), r\"(\\d{12})\\.xml$\", 1), \"yyyyMMddHHmm\").alias(\"file_timestamp\"),\n",
    "        col(\"xml_string\")\n",
    "    )\n",
    "    \n",
    "    sample_xml = df_base.select(\"xml_string\").first()[0]\n",
    "    xml_schema = schema_of_xml(sample_xml)\n",
    "    df_parsed_xml = df_base.withColumn(\"parsed\", from_xml(col(\"xml_string\"), xml_schema))\n",
    "    \n",
    "    timeseries_field = df_parsed_xml.select(\"parsed.TimeSeries\").schema.fields[0]\n",
    "    is_timeseries_array = str(timeseries_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "    if is_timeseries_array:\n",
    "        df_timeseries = df_parsed_xml.select(\n",
    "            col(\"filename\"),\n",
    "            col(\"ingestion_timestamp\"),\n",
    "            col(\"file_timestamp\"),\n",
    "            explode(col(\"parsed.TimeSeries\")).alias(\"timeseries\")\n",
    "        )\n",
    "    else:\n",
    "        df_timeseries = df_parsed_xml.select(\n",
    "            col(\"filename\"),\n",
    "            col(\"ingestion_timestamp\"),\n",
    "            col(\"file_timestamp\"),\n",
    "            explode(array(col(\"parsed.TimeSeries\"))).alias(\"timeseries\")\n",
    "        )\n",
    "    \n",
    "    df_timeseries_with_psrtype = df_timeseries.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"ingestion_timestamp\"),\n",
    "        col(\"file_timestamp\"),\n",
    "        col(\"timeseries.MktPSRType.psrType\").alias(\"psrType\"),\n",
    "        col(\"timeseries\")\n",
    "    )\n",
    "    \n",
    "    return df_timeseries_with_psrtype\n",
    "\n",
    "def extract_periods(df_timeseries: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract each Period from TimeSeries as a separate row.\n",
    "    \"\"\"\n",
    "    period_field = df_timeseries.select(\"timeseries.Period\").schema.fields[0]\n",
    "    is_period_array = str(period_field.dataType).startswith(\"ArrayType\")\n",
    "    \n",
    "    if is_period_array:\n",
    "        df_periods = df_timeseries.select(\n",
    "            col(\"filename\"),\n",
    "            col(\"ingestion_timestamp\"),\n",
    "            col(\"file_timestamp\"),\n",
    "            col(\"psrType\"),\n",
    "            explode(col(\"timeseries.Period\")).alias(\"period\")\n",
    "        )\n",
    "    else:\n",
    "        df_periods = df_timeseries.select(\n",
    "            col(\"filename\"),\n",
    "            col(\"ingestion_timestamp\"),\n",
    "            col(\"file_timestamp\"),\n",
    "            col(\"psrType\"),\n",
    "            explode(array(col(\"timeseries.Period\"))).alias(\"period\")\n",
    "        )\n",
    "    \n",
    "    df_periods_with_intervals = df_periods.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"ingestion_timestamp\"),\n",
    "        col(\"file_timestamp\"),\n",
    "        col(\"psrType\"),\n",
    "        to_timestamp(col(\"period.timeInterval.start\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_start\"),\n",
    "        to_timestamp(col(\"period.timeInterval.end\"), \"yyyy-MM-dd'T'HH:mm'Z'\").alias(\"interval_end\"),\n",
    "        col(\"period\")\n",
    "    )\n",
    "    \n",
    "    return df_periods_with_intervals\n",
    "\n",
    "def explode_points(df_periods: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Explode Points from each Period and calculate position_timestamp.\n",
    "    \"\"\"\n",
    "    df_points_exploded = df_periods.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"ingestion_timestamp\"),\n",
    "        col(\"file_timestamp\"),\n",
    "        col(\"psrType\"),\n",
    "        col(\"interval_start\"),\n",
    "        col(\"interval_end\"),\n",
    "        explode(col(\"period.Point\")).alias(\"point\")\n",
    "    )\n",
    "    \n",
    "    df_result = df_points_exploded.select(\n",
    "        col(\"filename\"),\n",
    "        col(\"ingestion_timestamp\"),\n",
    "        col(\"file_timestamp\"),\n",
    "        col(\"psrType\"),\n",
    "        col(\"interval_start\"),\n",
    "        col(\"interval_end\"),\n",
    "        col(\"point.position\").cast(\"int\").alias(\"position\"),\n",
    "        col(\"point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_result.withColumn(\n",
    "        \"position_timestamp\",\n",
    "        expr(\"interval_start + make_interval(0, 0, 0, 0, 0, (position - 1) * 15)\")\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def pivot_psrtype(df_final: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot the dataframe so each psrType becomes a column.\n",
    "    \"\"\"\n",
    "    df_pivoted = df_final.groupBy(\n",
    "        \"position_timestamp\",\n",
    "        \"filename\",\n",
    "        \"ingestion_timestamp\",\n",
    "        \"file_timestamp\",\n",
    "        \"interval_start\",\n",
    "        \"interval_end\"\n",
    "    ).pivot(\"psrType\").agg(first(\"quantity\"))\n",
    "    \n",
    "    df_pivoted = df_pivoted.orderBy(\"position_timestamp\")\n",
    "    \n",
    "    return df_pivoted\n",
    "\n",
    "# Define parse_xml_row function using modular functions\n",
    "def parse_xml_row(df_input: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parse XML data using modular functions and pivot on psrType.\n",
    "    \"\"\"\n",
    "    df_timeseries = extract_timeseries(df_input)\n",
    "    df_periods = extract_periods(df_timeseries)\n",
    "    df_points = explode_points(df_periods)\n",
    "    df_pivoted = pivot_psrtype(df_points)\n",
    "    \n",
    "    return df_pivoted\n",
    "\n",
    "# Define the process_batch function for foreachBatch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Process a micro-batch of bronze data and MERGE into silver table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing batch {batch_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    row_count = batch_df.count()\n",
    "    print(f\"Batch contains {row_count} bronze rows\")\n",
    "    \n",
    "    if row_count == 0:\n",
    "        print(\"Empty batch, skipping...\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    silver_table = DeltaTable.forName(spark, \"data.silver.silver_A75\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    for row in batch_df.toLocalIterator():\n",
    "        processed_count += 1\n",
    "        filename = row['filename']\n",
    "        print(f\"\\n  [{processed_count}/{row_count}] Processing: {filename}\")\n",
    "        \n",
    "        df_single_row = spark.createDataFrame([row], batch_df.schema)\n",
    "        df_parsed = parse_xml_row(df_single_row)\n",
    "        \n",
    "        # Build merge condition and update set dynamically based on columns\n",
    "        merge_condition = \"target.position_timestamp = source.position_timestamp\"\n",
    "        \n",
    "        # Get all columns for update\n",
    "        update_set = {col_name: f\"source.{col_name}\" for col_name in df_parsed.columns}\n",
    "        \n",
    "        silver_table.alias(\"target\").merge(\n",
    "            df_parsed.alias(\"source\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"source.file_timestamp > target.file_timestamp\",\n",
    "            set=update_set\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "        \n",
    "        print(f\"    ✓ MERGE completed\")\n",
    "    \n",
    "    print(f\"\\n✓ Batch {batch_id} completed: {processed_count} files processed\")\n",
    "\n",
    "print(\"✓ parse_xml_row and process_batch functions defined\")\n",
    "print(\"Ready to use with foreachBatch in streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6c8ba2-f1a6-4a47-ba9c-e13f6b98206c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create silver table"
    }
   },
   "outputs": [],
   "source": [
    "# Create silver table for A75 data with FLOAT columns\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE if not exists data.silver.silver_A75 (\n",
    "    position_timestamp TIMESTAMP,\n",
    "    filename STRING,\n",
    "    ingestion_timestamp TIMESTAMP,\n",
    "    file_timestamp TIMESTAMP,\n",
    "    interval_start TIMESTAMP,\n",
    "    interval_end TIMESTAMP,\n",
    "    B01 FLOAT,\n",
    "    B04 FLOAT,\n",
    "    B05 FLOAT,\n",
    "    B11 FLOAT,\n",
    "    B14 FLOAT,\n",
    "    B16 FLOAT,\n",
    "    B17 FLOAT,\n",
    "    B18 FLOAT,\n",
    "    B19 FLOAT,\n",
    "    B20 FLOAT\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Silver table for A75 generation load data with pivoted psrType columns'\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Silver table data.silver.silver_A75 created successfully with FLOAT columns\")\n",
    "print(\"\\nTable schema:\")\n",
    "spark.table(\"data.silver.silver_A75\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7397a06-6b4b-433b-bb4e-63bcd9535a5b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup streaming from bronze to silver"
    }
   },
   "outputs": [],
   "source": [
    "# Setup streaming from bronze to silver\n",
    "print(\"Setting up structured streaming...\")\n",
    "\n",
    "# Define checkpoint location\n",
    "checkpoint_location = \"/Volumes/source/source_schema/source_volume/checkpoints/A75_silver\"\n",
    "\n",
    "# Read bronze table as a stream\n",
    "df_bronze_stream = spark.readStream.table(\"data.bronze.bronze_A75\")\n",
    "\n",
    "print(f\"✓ Streaming source configured\")\n",
    "print(f\"Checkpoint location: {checkpoint_location}\")\n",
    "\n",
    "# Start the streaming query with foreachBatch\n",
    "print(\"\\nStarting streaming query...\")\n",
    "print(\"Mode: Batch (trigger availableNow - processes all available data once)\\n\")\n",
    "\n",
    "query = df_bronze_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to finish processing\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Streaming query completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll unprocessed bronze rows have been processed to silver.\")\n",
    "print(\"Next run will only process new rows added after this checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51327ed1-69cc-4d64-beff-9c468a888e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint_location = \"/Volumes/source/source_schema/source_volume/checkpoints/A75_silver\"\n",
    "# dbutils.fs.rm(checkpoint_location, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f87cdb-0090-4f8b-9ca6-5236189fb8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table data.silver.silver_A75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b4b34f5-8ec7-4deb-82c7-daca1be700ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4896449858612643,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_to_silver_A75",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
