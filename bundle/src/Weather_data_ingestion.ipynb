{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33115ef0-bd5e-40c3-8c55-889ed99d2d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "%pip install knmi-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4748058d-3213-4ec3-bd78-e2ffa54644fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "import knmi\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Calculate date range: from 2 weeks ago until now\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(weeks=40)\n",
    "\n",
    "# Format dates for API (YYYYMMDDHH)\n",
    "start_str = start_date.strftime(\"%Y%m%d%H\")\n",
    "end_str = end_date.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "print(f\"Fetching data from {start_date} to {end_date}\")\n",
    "\n",
    "# Fetch data from API\n",
    "hourly_df = knmi.get_hour_data_dataframe(\n",
    "    stations=[240],\n",
    "    start=start_str,\n",
    "    end=end_str\n",
    ")\n",
    "\n",
    "print(f\"Fetched {len(hourly_df)} rows from API\")\n",
    "\n",
    "# Reset index to make timestamp a regular column\n",
    "hourly_df = hourly_df.reset_index()\n",
    "\n",
    "# Add API query timestamp column\n",
    "api_query_timestamp = datetime.now()\n",
    "hourly_df['api_query_timestamp'] = api_query_timestamp\n",
    "\n",
    "print(f\"API query timestamp: {api_query_timestamp}\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(hourly_df)\n",
    "\n",
    "# Cast columns to match existing table schema (double columns)\n",
    "spark_df = spark_df.withColumn(\"T10N\", spark_df[\"T10N\"].cast(DoubleType())) \\\n",
    "                   .withColumn(\"N\", spark_df[\"N\"].cast(DoubleType())) \\\n",
    "                   .withColumn(\"WW\", spark_df[\"WW\"].cast(DoubleType())) \\\n",
    "                   .withColumn(\"Y\", spark_df[\"Y\"].cast(DoubleType()))\n",
    "\n",
    "# Insert data into Delta table\n",
    "spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data.bronze.weather_data_bronze\")\n",
    "\n",
    "print(f\"âœ… Successfully inserted {len(hourly_df)} rows into data.bronze.weather_data_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ea5f94e-88cc-4e07-a43b-d8b710f3dbbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8088173079012581,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Weather_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
